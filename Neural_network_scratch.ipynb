{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOB0aXz5HbwSMRMu+lKc+0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxB32/multi-purpose-neural-network/blob/main/Neural_network_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewAMoJIOQBvm"
      },
      "outputs": [],
      "source": [
        "# Load in necessary imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Neccessary functions for parts of a neural network"
      ],
      "metadata": {
        "id": "4n9iZZATXssw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Error functions"
      ],
      "metadata": {
        "id": "lppdaPURX20w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 - Regression cost function (mean squared error)"
      ],
      "metadata": {
        "id": "x99i4GlyfbAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MSE function\n",
        "def mean_squared_error(y_true, y_hat):\n",
        "  # Define length of data\n",
        "  m = len(y_true)\n",
        "  # Calculate error for individual parameters\n",
        "  loss_function = (y_true - y_hat)**2\n",
        "  # Calculate average to acquire cost\n",
        "  cost_function = np.mean(loss_function)\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "YB5zyeNSV1B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 - Binary classification cost function (Binary cross entropy)"
      ],
      "metadata": {
        "id": "adeHFLJ2hPUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true, y_hat):\n",
        "  # Extract length of input\n",
        "  m = len(y_true)\n",
        "  # Caluclate loss function (binary cross entropy error per instance)\n",
        "  loss_function = (y_true * np.log(y_hat) + (1 - y_true) *np.log(1 - y_hat))\n",
        "  # Take mean to acquire cost of all instances\n",
        "  cost_function = - np.mean(loss_function)\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "e5ad0p12hNY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 - Multi-class classification cost function (Classification cross entropy)"
      ],
      "metadata": {
        "id": "2JCiaMXdiWtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_pro(logits):\n",
        "  # Calculate softmax probabilities\n",
        "  expo_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "  softmax_prob = expo_logits / np.sum(expo_logits, axis=1, keepdims=True)\n",
        "  return softmax_prob\n",
        "\n",
        "\n",
        "def multi_cross_entropy(y_true, y_hat):\n",
        "\n",
        "  # Clip predictions to ensure no log(0)\n",
        "  y_hat = np.clip(y_hat, 1e-12, 1)\n",
        "\n",
        "  loss_function = -np.sum(y_true * np.log(y_hat), axis = 1)\n",
        "\n",
        "  cost_function = np.mean(loss_function)\n",
        "\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "BhYZXy9miWOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Derivatives of cost functions"
      ],
      "metadata": {
        "id": "jZdHNT1yk8Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 - Derivative of mean squared error"
      ],
      "metadata": {
        "id": "OBU9BLGBk_vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_MSE(y_hat, y_true):\n",
        "  m = y_true.shape[0]\n",
        "  delta_MSE = (2/m) * (y_hat - y_true)\n",
        "  return delta_MSE"
      ],
      "metadata": {
        "id": "Jj1nl6yuk7nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 - Derivative of binary cross entropy"
      ],
      "metadata": {
        "id": "7nSXrBuPlykS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_binary_XEntropy(y_hat, y_true):\n",
        "  delta_bin_XEntropy = (- y_true / y_hat) + ((1 - y_true) / (1 - y_hat))\n",
        "  return np.mean(delta_bin_XEntropy)"
      ],
      "metadata": {
        "id": "BHNEe4Gglx8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 - Derivative of classification cross entropy"
      ],
      "metadata": {
        "id": "kbUTqIBFmue4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_class_XEntropy(y_true, y_hat):\n",
        "  delta_class_XEntropy = -(y_true / y_hat)\n",
        "  return np.mean(delta_class_XEntropy)"
      ],
      "metadata": {
        "id": "sFWDCFCHmt7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 - Activation functions"
      ],
      "metadata": {
        "id": "3zS1mf89-2q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 - Sigmoid activation function"
      ],
      "metadata": {
        "id": "dJDwKOvE-63Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_func(z):\n",
        "  sigma_output = 1 / (1 + np.exp(-z))\n",
        "  return sigma_output"
      ],
      "metadata": {
        "id": "lTu2yY1O-2I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 - ReLu activation function"
      ],
      "metadata": {
        "id": "qNVabaQogBMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Relu_func_manual(z):\n",
        "  Relu_output = 0\n",
        "  if z > 0:\n",
        "    Relu_output = z\n",
        "  else:\n",
        "    Relu_output = 0\n",
        "  return Relu_output"
      ],
      "metadata": {
        "id": "EtEL5wJKgAgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Relu_func_auto(z):\n",
        "  Relu_output = np.maximum(0, z)\n",
        "  return Relu_output"
      ],
      "metadata": {
        "id": "uP8xUqXQiCuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 - Derivatives of activation functions"
      ],
      "metadata": {
        "id": "GWrh3lLBG8AM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 - Derivative of sigmoid function"
      ],
      "metadata": {
        "id": "A6E7GiLSG_OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_sigmoid(sigma_output):\n",
        "  delta_sigma = sigma_output(1 - sigma_output)\n",
        "  return delta_sigma"
      ],
      "metadata": {
        "id": "5JDegdFTHC03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 - Derivative of Relu activation function"
      ],
      "metadata": {
        "id": "2ce79WORHYX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_Relu(Relu_output):\n",
        "  if Relu_output > 1:\n",
        "    delta_Relu = 1\n",
        "  else:\n",
        "    delta_Relu = 0\n",
        "  return delta_Relu"
      ],
      "metadata": {
        "id": "wGM4wL0vHXuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 - Training neural network"
      ],
      "metadata": {
        "id": "1gtL0iqOrQ_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.1 - Forward pass"
      ],
      "metadata": {
        "id": "su5OLj6rrchg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define forward pass training function\n",
        "def forward_pass(n_neurons, n_features, n_instances, n_layers, X, activation_func):\n",
        "  # Define bias\n",
        "  b = np.ones((n_instances, 1))\n",
        "  # Append bias to input matrix\n",
        "  X_b = np.hstack([X, b])\n",
        "  activations = []\n",
        "\n",
        "  current_activations = X_b\n",
        "  activations.append(current_activation)\n",
        "  for L in range(n_layers):\n",
        "    weight_matrix = weight_matrices[L]\n",
        "    # Calculate activation\n",
        "    z = current_activation@weight_matrix\n",
        "    # Apply activation function to activation\n",
        "    if activation_func == 'sigmoid':\n",
        "      current_activation = sigmoid_func(z)\n",
        "    elif activation_func == 'Relu':\n",
        "      current_activation = Relu_func_auto(z)\n",
        "    else:\n",
        "      raise ValueError('Unknown activation function')\n",
        "    activations.append(current_activation)\n",
        "  return activations, weight_matrices"
      ],
      "metadata": {
        "id": "kIbPeT_Eq09M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 - Backpropagation"
      ],
      "metadata": {
        "id": "jrD0Tg0aId8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(logits, alpha, y_true, y_hat, activation_func, error_func, X, n_layers, activations, weight_matrices):\n",
        "  m = len(y_true)\n",
        "  # Compute gradient of loss with repsect to output (error)\n",
        "  if error_func == 'MSE':\n",
        "    delta_loss_activations = derivative_MSE(y_hat, y_true)\n",
        "  elif error_func == 'binary_Xentropy':\n",
        "    delta_loss_activations = derivative_binary_XEntropy(y_hat, y_true)\n",
        "  else:\n",
        "    delta_loss_activations = derivative_class_XEntropy(y_hat, y_true)\n",
        "  # Compute gradient of activations with respect to weight sums (z)\n",
        "  if activation_func == 'sigmoid':\n",
        "    delta_activations_WeightdSums = derivative_sigmoid(y_hat)\n",
        "  else:\n",
        "    delta_activations_WeightdSums = derivative_Relu(y_hat)\n",
        "  # Compute gradient of loss with respect to the weighted sums (z)\n",
        "  delta_current_layer = np.dot(delta_loss_activations, delta_activations_WeightdSums)\n",
        "  # Compute gradient of loss with respect to weights\n",
        "  delta_loss_weights = np.dot(delta_current_layer.T, X) / m\n",
        "  for L in reversed(range(1, n_layers)):\n",
        "    previous_activation = activations[L - 1]\n",
        "    if activation_func == 'sigmoid':\n",
        "      delta_previous_layer = np.dot(delta_current_layer, weight_matrix[L].T) * derivative_sigmoid(previous_activation)\n",
        "    else:\n",
        "      delta_previous_layer = np.dot(delta_current_layer, weight_matrix[L].T) * derivative_Relu(previous_activation)\n",
        "    weight_matrices[L] -= alpha * np.dot(previous_activation.T, delta_previous_layer)\n",
        "\n",
        "    delta_current_layer = delta_previous_layer\n",
        "    previous_activation = activations[L - 1]\n",
        "    return weight_matrices"
      ],
      "metadata": {
        "id": "3TViOhpF2Hm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.6 - Building neural network class"
      ],
      "metadata": {
        "id": "OB9bwLitXUYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the class for a multi-purpose neural network\n",
        "class multi_purpose_NN:\n",
        "    # Initialize the class with hyperparameters and network structure\n",
        "    def __init__(self, alpha, activation_func, error_func, n_neurons, n_features, n_instances, n_layers):\n",
        "        # Assign input arguments to instance variables\n",
        "        self.alpha = alpha\n",
        "        self.activation_func = activation_func\n",
        "        self.error_func = error_func\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_features = n_features\n",
        "        self.n_instances = n_instances\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Initialize weights dynamically using Xavier initialization\n",
        "        self.weights_matrices = []\n",
        "        input_dim = n_features + 1  # Add 1 to include the bias term\n",
        "        for layer in range(n_layers):\n",
        "            # Determine the output dimension based on layer type\n",
        "            if layer == n_layers - 1 and self.error_func == \"MSE\":\n",
        "                output_dim = 1  # Single output for regression\n",
        "            elif layer == n_layers - 1 and self.error_func == \"class_Xentropy\":\n",
        "                output_dim = 3  # Number of classes for classification\n",
        "            else:\n",
        "                output_dim = n_neurons\n",
        "\n",
        "            # Create a weight matrix with random initialization\n",
        "            weight_matrix = np.random.randn(input_dim, output_dim) * np.sqrt(1 / input_dim)\n",
        "            self.weights_matrices.append(weight_matrix)\n",
        "\n",
        "            # Update the input dimension for the next layer\n",
        "            input_dim = output_dim\n",
        "\n",
        "    # Perform a forward pass through the network\n",
        "    def forward_pass(self, X):\n",
        "        # Add a bias term to the input\n",
        "        b = np.ones((X.shape[0], 1))\n",
        "        X_b = np.hstack([X, b])\n",
        "        activations = []\n",
        "        current_activation = X_b\n",
        "        activations.append(current_activation)\n",
        "\n",
        "        # Compute activations for each layer\n",
        "        for L in range(self.n_layers):\n",
        "            weight_matrix = self.weights_matrices[L]\n",
        "            z = current_activation @ weight_matrix\n",
        "\n",
        "            # Apply the appropriate activation function\n",
        "            if L == self.n_layers - 1 and self.error_func == 'class_Xentropy':\n",
        "                current_activation = self.softmax_func(z)\n",
        "            elif self.activation_func == 'sigmoid':\n",
        "                current_activation = self.sigmoid_func(z)\n",
        "            elif self.activation_func == 'Relu':\n",
        "                current_activation = self.Relu_func_auto(z)\n",
        "            else:\n",
        "                raise ValueError('Unknown activation function')\n",
        "\n",
        "            activations.append(current_activation)\n",
        "        return activations\n",
        "\n",
        "    # Perform backpropagation to update weights\n",
        "    def backpropagation(self, X, y_true, y_hat, activations):\n",
        "        # Calculate the number of training examples\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        # Compute the gradient for the output layer based on the loss function\n",
        "        if self.error_func == 'MSE':\n",
        "            delta_loss_activations = self.derivative_MSE(y_hat, y_true)\n",
        "            if self.activation_func == 'sigmoid':\n",
        "                delta_activations_WeightdSums = self.derivative_sigmoid(y_hat)\n",
        "            elif self.activation_func == 'Relu':\n",
        "                delta_activations_WeightdSums = np.where(y_hat > 0, 1, 0)\n",
        "            else:\n",
        "                raise ValueError('Unsupported activation function for MSE')\n",
        "            delta_current_layer = delta_loss_activations * delta_activations_WeightdSums\n",
        "        elif self.error_func == 'class_Xentropy':\n",
        "            delta_current_layer = y_hat - y_true\n",
        "        elif self.error_func == 'binary_Xentropy':\n",
        "            delta_loss_activations = self.derivative_binary_XEntropy(y_hat, y_true)\n",
        "            if self.activation_func == 'sigmoid':\n",
        "                delta_current_layer = delta_loss_activations * self.derivative_sigmoid(y_hat)\n",
        "            else:\n",
        "                raise ValueError('Unsupported activation function for binary cross-entropy')\n",
        "        else:\n",
        "            raise ValueError('Unsupported error function')\n",
        "\n",
        "        # Update the weights for the output layer\n",
        "        gradient_output_layer = np.dot(activations[-2].T, delta_current_layer) / m\n",
        "        self.weights_matrices[-1] -= self.alpha * gradient_output_layer\n",
        "\n",
        "        # Backpropagate through the hidden layers\n",
        "        for L in reversed(range(self.n_layers - 1)):\n",
        "            delta_previous_layer = np.dot(delta_current_layer, self.weights_matrices[L + 1].T)\n",
        "            if self.activation_func == 'sigmoid':\n",
        "                delta_previous_layer *= self.derivative_sigmoid(activations[L + 1])\n",
        "            elif self.activation_func == 'Relu':\n",
        "                delta_previous_layer *= np.where(activations[L + 1] > 0, 1, 0)\n",
        "            else:\n",
        "                raise ValueError('Unsupported activation function for hidden layers')\n",
        "\n",
        "            gradient_hidden_layer = np.dot(activations[L].T, delta_previous_layer) / m\n",
        "            self.weights_matrices[L] -= self.alpha * gradient_hidden_layer\n",
        "            delta_current_layer = delta_previous_layer\n",
        "\n",
        "    # Train the neural network\n",
        "    def train(self, X, y, epochs):\n",
        "        # Ensure correct format for the labels\n",
        "        if self.error_func == \"class_Xentropy\":\n",
        "            if len(y.shape) != 2 or y.shape[1] != self.weights_matrices[-1].shape[1]:\n",
        "                raise ValueError(f\"Expected one-hot encoded labels with {self.weights_matrices[-1].shape[1]} classes, but got shape {y.shape}\")\n",
        "        elif self.error_func == \"MSE\":\n",
        "            y = y.reshape(-1, 1)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Perform forward pass\n",
        "            activations = self.forward_pass(X)\n",
        "            y_hat = activations[-1]\n",
        "\n",
        "            # Check alignment of predicted and true labels\n",
        "            if y.shape != y_hat.shape:\n",
        "                raise ValueError(f\"Shape mismatch: y_true shape = {y.shape}, y_hat shape = {y_hat.shape}\")\n",
        "\n",
        "            # Perform backpropagation\n",
        "            self.backpropagation(X, y, y_hat, activations)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = self.compute_loss(y_hat, y)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    # Make predictions using the trained model\n",
        "    def predict(self, X):\n",
        "        activations = self.forward_pass(X)\n",
        "        return activations[-1]\n",
        "\n",
        "    # Compute the loss based on the error function\n",
        "    def compute_loss(self, y_hat, y_true):\n",
        "        if self.error_func == 'MSE':\n",
        "            loss = np.mean((y_hat - y_true) ** 2)\n",
        "        elif self.error_func == 'class_Xentropy':\n",
        "            y_hat = np.clip(y_hat, 1e-12, 1 - 1e-12)\n",
        "            loss = -np.mean(np.sum(y_true * np.log(y_hat), axis=1))\n",
        "        elif self.error_func == 'binary_Xentropy':\n",
        "            y_hat = np.clip(y_hat, 1e-12, 1 - 1e-12)\n",
        "            loss = -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
        "        else:\n",
        "            raise ValueError('Unknown error function type')\n",
        "        return loss\n",
        "\n",
        "    # Define activation and helper functions\n",
        "    def sigmoid_func(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def Relu_func_auto(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def softmax_func(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def derivative_MSE(self, y_hat, y_true):\n",
        "        return 2 * (y_hat - y_true) / y_true.shape[0]\n",
        "\n",
        "    def derivative_sigmoid(self, sigma_output):\n",
        "        return sigma_output * (1 - sigma_output)\n",
        "\n",
        "    # Calculate accuracy for classification tasks\n",
        "    def calculate_accuracy(self, y_true, y_pred):\n",
        "        correct_predictions = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))\n",
        "        total_predictions = y_true.shape[0]\n",
        "        return correct_predictions / total_predictions\n",
        "\n",
        "    # Calculate precision for classification tasks\n",
        "    def calculate_precision(self, y_true, y_pred):\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true_classes = np.argmax(y_true, axis=1)\n",
        "        precision_scores = []\n",
        "        for class_label in np.unique(y_true_classes):\n",
        "            true_positives = np.sum((y_pred_classes == class_label) & (y_true_classes == class_label))\n",
        "            predicted_positives = np.sum(y_pred_classes == class_label)\n",
        "            precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
        "            precision_scores.append(precision)\n",
        "        return precision_scores\n",
        "\n",
        "    # Calculate Root Mean Square Error\n",
        "    def calculate_rmse(self, y_true, y_pred):\n",
        "        return np.sqrt(np.mean((y_true - y_pred) ** 2))"
      ],
      "metadata": {
        "id": "WxvngKceTQcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 - Loading in data for testing"
      ],
      "metadata": {
        "id": "C2bUNvSHPmUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 - Classification data"
      ],
      "metadata": {
        "id": "P8TcTwLSPsAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4ryPjvglPrmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 - Regression data"
      ],
      "metadata": {
        "id": "-Fl_w7YOPyX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_reg = scaler.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler.transform(X_test_reg)\n"
      ],
      "metadata": {
        "id": "DxoxfyB3pX1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 - Neural network initial training and testing"
      ],
      "metadata": {
        "id": "J2IVt6MuP8Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Basic classification neural network (3 neurons and 2 layers)"
      ],
      "metadata": {
        "id": "phKZ7a0vQBOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=3, n_features=4, n_instances=120, n_layers=2)\n",
        "\n",
        "nn_classification.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification.predict(X_test_class)\n",
        "accuracy = nn_classification.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuCc9MNyQArc",
        "outputId": "224ee13f-b56a-469a-a32b-821e1352e8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.5653303104659433\n",
            "Epoch 100, Loss: 0.8809943450023492\n",
            "Epoch 200, Loss: 0.8228411751004551\n",
            "Epoch 300, Loss: 0.79423159033933\n",
            "Epoch 400, Loss: 0.7809486708292527\n",
            "Epoch 500, Loss: 0.7717379580150736\n",
            "Epoch 600, Loss: 0.7645456047606151\n",
            "Epoch 700, Loss: 0.7588677373151416\n",
            "Epoch 800, Loss: 0.7544542445798861\n",
            "Epoch 900, Loss: 0.7509829080820993\n",
            "Classification Accuracy: 0.3333333333333333\n",
            "Classification Precision (Per Class): [0.3333333333333333, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Basic regression neural network (3 neurons and 2 layers)"
      ],
      "metadata": {
        "id": "2Diqzc5cQT0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before initializing the neural network\n",
        "print(f\"X_train_reg shape: {X_train_reg.shape}\")  # Print the shape of the training data\n",
        "print(f\"Number of features in X_train_reg: {X_train_reg.shape[1]}\")  # Print the number of features\n",
        "\n",
        "nn_regression = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=3,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "\n",
        "# After initialization, print the network configuration\n",
        "print(f\"n_features in neural network: {nn_regression.n_features}\")\n",
        "print(f\"Expected input size in neural network: {nn_regression.n_features + 1} (including bias term)\")\n",
        "print(f\"Weight matrix for Layer 0: {nn_regression.weights_matrices[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erHfbradQS6B",
        "outputId": "2145208d-6cbe-49bc-f370-9a30ea71edb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_reg shape: (16512, 8)\n",
            "Number of features in X_train_reg: 8\n",
            "n_features in neural network: 8\n",
            "Expected input size in neural network: 9 (including bias term)\n",
            "Weight matrix for Layer 0: (9, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "r1_pIpbUbMl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c4850b-3123-4fef-9bdd-89178ddce79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.5685820538712556\n",
            "Epoch 100, Loss: 3.568557899856682\n",
            "Epoch 200, Loss: 3.56853374643444\n",
            "Epoch 300, Loss: 3.568509593604529\n",
            "Epoch 400, Loss: 3.568485441366948\n",
            "Epoch 500, Loss: 3.5684612897216965\n",
            "Epoch 600, Loss: 3.568437138668775\n",
            "Epoch 700, Loss: 3.5684129882081823\n",
            "Epoch 800, Loss: 3.5683888383399185\n",
            "Epoch 900, Loss: 3.568364689063982\n",
            "Root Mean Square Error (RMSE): 1.8693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Neuron experimentation"
      ],
      "metadata": {
        "id": "5haYS0T2nYlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the number of layers will be held constant at 2, all experiments won't be optimal due to timing scenarios and reducing code to impleemnt a random search or similar optimality algorithm"
      ],
      "metadata": {
        "id": "0fvsAQhhns5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 - Classification 12 neurons"
      ],
      "metadata": {
        "id": "5gjo89_BoFi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_12 = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=12, n_features=4, n_instances=120, n_layers=2)\n",
        "\n",
        "nn_classification_12.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_12.predict(X_test_class)\n",
        "accuracy = nn_classification_12.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_12.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrnCHwQSMawM",
        "outputId": "f3a7184a-0cc3-4312-aa15-88b7e13c5dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.017796718416685\n",
            "Epoch 100, Loss: 0.6014516353211348\n",
            "Epoch 200, Loss: 0.4619003366914164\n",
            "Epoch 300, Loss: 0.3968984750901633\n",
            "Epoch 400, Loss: 0.351598377383943\n",
            "Epoch 500, Loss: 0.31332243031273693\n",
            "Epoch 600, Loss: 0.27952591827361256\n",
            "Epoch 700, Loss: 0.2501118469752322\n",
            "Epoch 800, Loss: 0.22487008110492285\n",
            "Epoch 900, Loss: 0.2036461180674146\n",
            "Classification Accuracy: 1.0\n",
            "Classification Precision (Per Class): [1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Classiciation 24 neurons"
      ],
      "metadata": {
        "id": "IR67vu9NJqqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_24 = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=24, n_features=4, n_instances=120, n_layers=2)\n",
        "\n",
        "nn_classification_24.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_24.predict(X_test_class)\n",
        "accuracy = nn_classification_24.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_24.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "id": "ytptTcTyqUuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce124e2e-d058-4b0c-cd51-2ea3bbd66844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.2931964092354624\n",
            "Epoch 100, Loss: 0.4848314681191515\n",
            "Epoch 200, Loss: 0.37761436756651345\n",
            "Epoch 300, Loss: 0.31088110277585995\n",
            "Epoch 400, Loss: 0.26341045513102723\n",
            "Epoch 500, Loss: 0.22843406177669465\n",
            "Epoch 600, Loss: 0.20210841633091883\n",
            "Epoch 700, Loss: 0.1819090348521541\n",
            "Epoch 800, Loss: 0.16611343657162753\n",
            "Epoch 900, Loss: 0.15353808297188984\n",
            "Classification Accuracy: 0.9333333333333333\n",
            "Classification Precision (Per Class): [1.0, 0.8181818181818182, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - Classification 36 neurons"
      ],
      "metadata": {
        "id": "BEO1aNDkKDZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_36 = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=36, n_features=4, n_instances=120, n_layers=2)\n",
        "\n",
        "nn_classification_36.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_36.predict(X_test_class)\n",
        "accuracy = nn_classification_36.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_36.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrQlWpSfJ5iD",
        "outputId": "a147542e-d3b2-4f74-986d-2ab5fc034d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.495927703653554\n",
            "Epoch 100, Loss: 0.38598476829362494\n",
            "Epoch 200, Loss: 0.30468565335109654\n",
            "Epoch 300, Loss: 0.2546955730392503\n",
            "Epoch 400, Loss: 0.21858716966245478\n",
            "Epoch 500, Loss: 0.19179413422632013\n",
            "Epoch 600, Loss: 0.17165003704837078\n",
            "Epoch 700, Loss: 0.15629618324307384\n",
            "Epoch 800, Loss: 0.14421738262994588\n",
            "Epoch 900, Loss: 0.13452069516147794\n",
            "Classification Accuracy: 1.0\n",
            "Classification Precision (Per Class): [1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 - Regression 12 neurons"
      ],
      "metadata": {
        "id": "kHWIa9B4K1MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_12 = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=12,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_12.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_12.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_12.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQe8MgjCKPFN",
        "outputId": "ecf6bc88-7c00-401f-8900-caac78ca437d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.223217447647459\n",
            "Epoch 100, Loss: 4.2231032363011645\n",
            "Epoch 200, Loss: 4.2229890209747865\n",
            "Epoch 300, Loss: 4.222874801670544\n",
            "Epoch 400, Loss: 4.22276057839067\n",
            "Epoch 500, Loss: 4.222646351137383\n",
            "Epoch 600, Loss: 4.222532119912916\n",
            "Epoch 700, Loss: 4.222417884719492\n",
            "Epoch 800, Loss: 4.22230364555934\n",
            "Epoch 900, Loss: 4.22218940243469\n",
            "Root Mean Square Error (RMSE): 2.0324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 - Regression 24 neurons"
      ],
      "metadata": {
        "id": "j0G7uGH3LSH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_24 = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=24,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_24.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_24.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_24.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760EOZL3LIka",
        "outputId": "fbb73655-7cd7-44f7-9c9d-f8bf881c5631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.075664801932745\n",
            "Epoch 100, Loss: 4.075428059074445\n",
            "Epoch 200, Loss: 4.075191314185063\n",
            "Epoch 300, Loss: 4.074954567286191\n",
            "Epoch 400, Loss: 4.074717818399423\n",
            "Epoch 500, Loss: 4.07448106754635\n",
            "Epoch 600, Loss: 4.074244314748569\n",
            "Epoch 700, Loss: 4.074007560027675\n",
            "Epoch 800, Loss: 4.073770803405266\n",
            "Epoch 900, Loss: 4.0735340449029405\n",
            "Root Mean Square Error (RMSE): 1.9891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 - Regression 36 neurons"
      ],
      "metadata": {
        "id": "5S6yM9pLLevF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_36 = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=36,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=2\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_36.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_36.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_36.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZo06z23LYuT",
        "outputId": "e016a1e3-cb69-4c66-eea5-1b13ae371e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.9603495467145997\n",
            "Epoch 100, Loss: 3.960018876702875\n",
            "Epoch 200, Loss: 3.959688223536534\n",
            "Epoch 300, Loss: 3.9593575872738995\n",
            "Epoch 400, Loss: 3.9590269679732777\n",
            "Epoch 500, Loss: 3.9586963656929575\n",
            "Epoch 600, Loss: 3.9583657804912113\n",
            "Epoch 700, Loss: 3.958035212426296\n",
            "Epoch 800, Loss: 3.957704661556452\n",
            "Epoch 900, Loss: 3.9573741279399015\n",
            "Root Mean Square Error (RMSE): 1.9652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Layers experimentation"
      ],
      "metadata": {
        "id": "0hn3fwOpL_XA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the neuron experiment the purpose of this experiment is more to see if the nuerl network algorithm can indeed update dynamically the number of layers to create different neural network architectures rather than fidning optimal solutions for the datasets."
      ],
      "metadata": {
        "id": "kDyWtoPdMCgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 - Classification 3 layer experiment, where we will use max number of neurons experimneted with at a number of 36"
      ],
      "metadata": {
        "id": "L0fZ5W0iMXfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_3Layer = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=36, n_features=4, n_instances=120, n_layers=3)\n",
        "\n",
        "nn_classification_3Layer.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_3Layer.predict(X_test_class)\n",
        "accuracy = nn_classification_3Layer.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_3Layer.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG89iX_sLqAr",
        "outputId": "0b195ee1-ab14-480d-c11d-e1d7ac7774e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3424253815457083\n",
            "Epoch 100, Loss: 0.3756079093160711\n",
            "Epoch 200, Loss: 0.2505125214884822\n",
            "Epoch 300, Loss: 0.1831070986719854\n",
            "Epoch 400, Loss: 0.1440885996149338\n",
            "Epoch 500, Loss: 0.12063165998329088\n",
            "Epoch 600, Loss: 0.10569302065102656\n",
            "Epoch 700, Loss: 0.09564526078486689\n",
            "Epoch 800, Loss: 0.0885714126997005\n",
            "Epoch 900, Loss: 0.08340495421385977\n",
            "Classification Accuracy: 0.9333333333333333\n",
            "Classification Precision (Per Class): [1.0, 0.8888888888888888, 0.9090909090909091]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 - Classification 4 layers"
      ],
      "metadata": {
        "id": "vudPEGYzM9DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_4Layer = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=36, n_features=4, n_instances=120, n_layers=4)\n",
        "\n",
        "nn_classification_4Layer.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_4Layer.predict(X_test_class)\n",
        "accuracy = nn_classification_4Layer.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_4Layer.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd9Il5PJMzjT",
        "outputId": "3ed3b994-88cc-4e88-aceb-06f6c5604943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.2092956482604724\n",
            "Epoch 100, Loss: 0.43444877206992744\n",
            "Epoch 200, Loss: 0.2727077098497286\n",
            "Epoch 300, Loss: 0.1729378482208787\n",
            "Epoch 400, Loss: 0.12798445878362424\n",
            "Epoch 500, Loss: 0.10637806059948171\n",
            "Epoch 600, Loss: 0.09468986081502301\n",
            "Epoch 700, Loss: 0.08763785420755796\n",
            "Epoch 800, Loss: 0.08298288971957914\n",
            "Epoch 900, Loss: 0.07968016975219049\n",
            "Classification Accuracy: 0.9333333333333333\n",
            "Classification Precision (Per Class): [1.0, 0.8181818181818182, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 - Classification 5 layers"
      ],
      "metadata": {
        "id": "mde6IPdsNK5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "nn_classification_5Layer = multi_purpose_NN(alpha=0.01, activation_func='Relu', error_func='class_Xentropy',\n",
        "                                     n_neurons=36, n_features=4, n_instances=120, n_layers=5)\n",
        "\n",
        "nn_classification_5Layer.train(X_train_class, y_train_class, epochs=1000)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_test_class = nn_classification_5Layer.predict(X_test_class)\n",
        "accuracy = nn_classification_5Layer.calculate_accuracy(y_test_class, y_pred_test_class)\n",
        "precision = nn_classification_5Layer.calculate_precision(y_test_class, y_pred_test_class)\n",
        "\n",
        "print(f\"Classification Accuracy: {accuracy}\")\n",
        "print(f\"Classification Precision (Per Class): {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADy3YVt4NH2R",
        "outputId": "48d7215c-a236-4a38-be48-fcced090eb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.1803129538229755\n",
            "Epoch 100, Loss: 0.44900718871160916\n",
            "Epoch 200, Loss: 0.20949110081944558\n",
            "Epoch 300, Loss: 0.12738373601906858\n",
            "Epoch 400, Loss: 0.10020873425958277\n",
            "Epoch 500, Loss: 1.0870197238116712\n",
            "Epoch 600, Loss: 0.1909030521339265\n",
            "Epoch 700, Loss: nan\n",
            "Epoch 800, Loss: nan\n",
            "Epoch 900, Loss: nan\n",
            "Classification Accuracy: 0.3333333333333333\n",
            "Classification Precision (Per Class): [0.3333333333333333, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 - Regression 3 layers"
      ],
      "metadata": {
        "id": "TMvYBgU7N25y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_3Layer = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=36,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=3\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_3Layer.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_3Layer.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_3Layer.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StfpYb68NVN5",
        "outputId": "adeccb47-0bce-441e-84c3-d8bec0cb8c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.703758850431712\n",
            "Epoch 100, Loss: 3.703410713144286\n",
            "Epoch 200, Loss: 3.7030626479963162\n",
            "Epoch 300, Loss: 3.7027146550377545\n",
            "Epoch 400, Loss: 3.702366734318479\n",
            "Epoch 500, Loss: 3.7020188858882963\n",
            "Epoch 600, Loss: 3.7016711097969393\n",
            "Epoch 700, Loss: 3.701323406094069\n",
            "Epoch 800, Loss: 3.700975774829271\n",
            "Epoch 900, Loss: 3.7006282160520634\n",
            "Root Mean Square Error (RMSE): 1.9040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 - Regression 4 layers"
      ],
      "metadata": {
        "id": "7FheOYrvRWfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_4Layer = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=36,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=4\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_4Layer.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_4Layer.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_4Layer.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lOabFecOKY5",
        "outputId": "c73f4060-7db0-4a92-e824-f68511d29210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.9695291215442774\n",
            "Epoch 100, Loss: 3.969155353652236\n",
            "Epoch 200, Loss: 3.9687816044840267\n",
            "Epoch 300, Loss: 3.9684078741225504\n",
            "Epoch 400, Loss: 3.968034162650691\n",
            "Epoch 500, Loss: 3.9676604701513063\n",
            "Epoch 600, Loss: 3.967286796707237\n",
            "Epoch 700, Loss: 3.966913142401296\n",
            "Epoch 800, Loss: 3.9665395073162766\n",
            "Epoch 900, Loss: 3.966165891534948\n",
            "Root Mean Square Error (RMSE): 1.9711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 - Regression 5 layers"
      ],
      "metadata": {
        "id": "tvVqELVWRaWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_regression_5Layer = multi_purpose_NN(\n",
        "    alpha=0.01,\n",
        "    activation_func='sigmoid',\n",
        "    error_func='MSE',\n",
        "    n_neurons=36,\n",
        "    n_features=8,\n",
        "    n_instances=X_train_reg.shape[0],\n",
        "    n_layers=5\n",
        ")\n",
        "\n",
        "# Train the regression model\n",
        "epochs = 1000\n",
        "nn_regression_5Layer.train(X_train_reg, y_train_reg, epochs)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "y_pred_reg = nn_regression_5Layer.predict(X_test_reg)\n",
        "\n",
        "# Calculate RMSE using the method in your class\n",
        "rmse = nn_regression_5Layer.calculate_rmse(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Display RMSE\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-a-pSfsQsaw",
        "outputId": "1af743cf-2a9a-4cd5-f01b-baca820712dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 3.9400443333349826\n",
            "Epoch 100, Loss: 3.939631410111724\n",
            "Epoch 200, Loss: 3.9392185183926847\n",
            "Epoch 300, Loss: 3.938805658288919\n",
            "Epoch 400, Loss: 3.938392829911435\n",
            "Epoch 500, Loss: 3.937980033371193\n",
            "Epoch 600, Loss: 3.9375672687791075\n",
            "Epoch 700, Loss: 3.9371545362460476\n",
            "Epoch 800, Loss: 3.9367418358828328\n",
            "Epoch 900, Loss: 3.9363291678002392\n",
            "Root Mean Square Error (RMSE): 1.9636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mddn8asqRiYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}